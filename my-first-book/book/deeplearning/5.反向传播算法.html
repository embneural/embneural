<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>反向传播算法介绍</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../deeplearning/1.深度学习入门介绍.html">深度学习简要介绍</a></li><li class="chapter-item expanded "><a href="../deeplearning/2.手写数字识别.html"><strong aria-hidden="true">1.</strong> 从手写数字识别入门深度学习</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../deeplearning/3.多层感知机(MLP)解析.html"><strong aria-hidden="true">1.1.</strong> 多层感知机(MLP)解析</a></li><li class="chapter-item expanded "><a href="../deeplearning/4.交叉熵损失函数.html"><strong aria-hidden="true">1.2.</strong> 交叉熵损失函数介绍</a></li><li class="chapter-item expanded "><a href="../deeplearning/5.反向传播算法.html" class="active"><strong aria-hidden="true">1.3.</strong> 反向传播算法介绍</a></li></ol></li><li class="chapter-item expanded "><a href="../misc/index.html"><strong aria-hidden="true">2.</strong> 杂项记录</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../misc/1.vscode_ssh.html"><strong aria-hidden="true">2.1.</strong> VSCode远程连接实验室windows电脑</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h2 id="什么是反向传播"><a class="header" href="#什么是反向传播"><strong>什么是反向传播</strong></a></h2>
<p>反向传播（Backpropagation）是一种用于训练神经网络的优化算法，它通过计算损失函数对神经网络参数的梯度来更新参数，从而最小化损失函数。反向传播算法是一种基于梯度下降的优化算法，它可以有效地优化多层神经网络中的参数，让模型具备学习能力。</p>
<h2 id="sofmax求导"><a class="header" href="#sofmax求导"><strong>Sofmax求导</strong></a></h2>
<p>我们有 模型的逻辑输出 $ z = [z_1, z_2, ..., z_n]^T$, Softmax的输出为
$s = [s_1, s_2, ..., s_n]^T$</p>
<p>$$
\begin{equation}
s_i = \frac{e^{z_i}}{\sum_{j=1}^{n}{e^{z_j}}  }
\end{equation}
$$
然后我们要求解 $\frac{\partial{s}}{\partial{z}}$， 即sotmax的导数。Softmax的导数可以用Jacobian矩阵来表示，如下式所示。
$$
\begin{equation}
\begin{split}</p>
<p>\frac{\partial{s}}{\partial{z}} = 
\begin{bmatrix}
\frac{\partial{s_1}}{\partial{z}}  \[6pt]
\frac{\partial{s_2}}{\partial{z}} \[6pt]
\vdots \[6pt]
\frac{\partial{s_n}}{\partial{z}} \[6pt]
\end{bmatrix}</p>
<p>&amp;=
\begin{bmatrix}
\frac{\partial{s_1}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_1}}{\partial{z_n}}\[6pt]
\frac{\partial{s_2}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_2}}{\partial{z_n}}\[6pt]
\vdots &amp; \frac{\partial{s_i}}{\partial{z_j}}  &amp; \vdots \[6pt]
\frac{\partial{s_n}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_n}}{\partial{z_n}}\[6pt]
\end{bmatrix}, (i,j \in [1, n])
\end{split}
\end{equation}
$$</p>
<p>我们首先求解 $\frac{\partial{s_i}}{\partial{z_j}}$。
但是因为有指数e，不好求导，为了方便求导，可以先加入log (默认log的底数为e) 即
$$
\begin{equation}
\begin{split}
log(s_i) &amp;= log(\frac{e^{z_i}}{\sum_{j=1}^{n}{e^{z_j}}  }) \
&amp;= z_i - log(\sum_{j=1}^{n}{e^{z_j}}) \
\end{split}
\end{equation}
$$</p>
<p>然后我们求解  $\frac{\partial{log(s_i)}}{\partial{z_j}}$</p>
<p>$$
\begin{equation}
\begin{split}
\frac{\partial{log(s_i)}}{\partial{z_j}} = \frac{\partial{log(s_i)}}{\partial{s_i}} \cdot \frac{\partial{ s_i }}{\partial{z_j}}
= \frac{1}{ s_i } \cdot \frac{\partial{ s_i }}{\partial{z_j}}
\end{split}
\end{equation}
$$
因此有$ \frac{\partial{ s_i }}{\partial{z_j}} = \frac{\partial{log(s_i)}}{\partial{s_i}} \cdot  s_i $。 所以我们要求下面的式子
$$
\begin{equation}
\begin{split}
\frac{\partial{log(s_i)}}{\partial{z_j}} 
&amp;= \frac{\partial{ (z_i - log(\sum_{j=1}^{n}{e^{z_j}}) )  }}{\partial{z_j}}\
&amp;= \frac{\partial{ z_i}}{\partial{z_j}} - \frac{\partial{ log(\sum_{j=1}^{n}{e^{z_j}}) }}{\partial{z_j}} \
&amp;= \frac{\partial{ z_i}}{\partial{z_j}}  - \frac{\partial{ log(\sum_{j=1}^{n}{e^{z_j}}) }}{\partial{(\sum_{j=1}^{n}{e^{z_j}}) }} \cdot 
\frac{\partial{(\sum_{j=1}^{n}{e^{z_j}}) }}{\partial{z_j}} \
&amp;= \frac{\partial{ z_i}}{\partial{z_j}} - \frac{e^{z_j}}{\sum_{j=1}^{n}{e^{z_j}}}\
&amp;= \frac{\partial{ z_i}}{\partial{z_j}} - s_j
\end{split}
\end{equation}
$$</p>
<p>之后我们就可以得到
$$
\begin{equation}
\begin{split}
\frac{\partial{s_i}}{\partial{z_j}} &amp;=  s_i \cdot \frac{\partial{ z_i}}{\partial{z_j}} - s_i s_j
\end{split}
\end{equation}
$$</p>
<p>最后我们就可以得到, sotfma的导数。表达式如下：
$$
\begin{equation}
\begin{split}</p>
<p>\frac{\partial{s}}{\partial{z}} = 
\begin{bmatrix}
\frac{\partial{s_1}}{\partial{z}}  \[6pt]
\frac{\partial{s_2}}{\partial{z}} \[6pt]
\vdots \[6pt]
\frac{\partial{s_n}}{\partial{z}} \[6pt]
\end{bmatrix}</p>
<p>&amp;=
\begin{bmatrix}
\frac{\partial{s_1}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_1}}{\partial{z_n}}\[6pt]
\frac{\partial{s_2}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_2}}{\partial{z_n}}\[6pt]
\vdots &amp; \frac{\partial{s_i}}{\partial{z_j}}  &amp; \vdots \[6pt]
\frac{\partial{s_n}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_n}}{\partial{z_n}}\[6pt]
\end{bmatrix}, (i,j \in [1, n])
\end{split}
\end{equation}
$$</p>
<p>$$
\frac{\partial{s_i}}{\partial{z_j}}  = 
\begin{cases}
s_i - {s_i}{s_j} &amp; \text{if } (i = j ) \
-{s_i}{s_j} &amp; \text{if } (i \neq j) \
\end{cases}
$$</p>
<h2 id="反向传播的步骤"><a class="header" href="#反向传播的步骤"><strong>反向传播的步骤</strong></a></h2>
<p>让我们继续以这个MLP为例，来详细解析反向传播算法的具体计算过程。
<img src="/images/3.png" alt="" /></p>
<ul>
<li>
<p>前向传播：将展平为1维的图片 $x \in \mathbb{R}^m$ 送入到MLP中去，计算出MLP的输出结果 $\hat{y} \in \mathbb{R}^{10}$。这个过程用公式来表示就是
$$
\begin{equation}
\begin{aligned}
h &amp;= \sigma(W_1 x) + b_1 \
z &amp;=  W h + b \
\hat{y} &amp;= softmax(z) = \frac{e^{z}}{\text{sum}({e^z}) }</p>
<p>\end{aligned}
\end{equation}
$$
其中 $W_1 \in \mathbb{R}^{64 \times m} $ 表示输入层和隐藏层之间的权重参数， $b_1 \in \mathbb{R}^{64} $ 表示隐藏层中的这64个神经元的偏置。$h \in \mathbb{R}^{64}$表示隐藏层所有神经元的输出，是一个64维度的向量。
$W \in \mathbb{R}^{10 \times 64} $ 表示输入层和隐藏层之间的权重参数， $b \in \mathbb{R}^{10} $ 表示隐藏层中的这10个神经元的偏置。$z \in \mathbb{R}^{10}$表示输出层所有神经元的逻辑输出，<strong>在Pytorch中使用nn.CrossEntropyLoss()时，一定要注意把逻辑输出 $z$送给交叉熵函数，因为该函数的内部已经调用了softmax函数</strong>。
$W_1$，$W$，$b_1$ 和 $b$ 都是通过训练更新得来的。$\sigma$ 是激活函数。</p>
</li>
<li>
<p>计算梯度</p>
<p>要想计算计算梯度，那么必须要获取到损失函数的损失值。我们在上一节中已经详细地解说了pytorch中的交叉熵函数包括3个部分，即 首先把MLP的逻辑输出通过sotfmax函数激活，得到概率预测值 $\hat{y}$，然后将真实标签转换为one-hot编码 $y$，最后计算 $\hat{y}$ 和 $y$ 之间的NLL损失，如下式所示。 
$$
\begin{equation}
L(y, \hat{y}) = -\sum_{i=1}^k y_i \log \hat{y}_i<br />
\end{equation}
$$</p>
<p>然后我们就可以通过NLL loss来获取到梯度值。
那么求谁的梯度呢？因为我们要优化的是$W_1, W, b_1, b$ 的参数值，所以当然试求 NLL Loss对 $W_1, W, b_1, b$ 的梯度。矩阵的偏导数求解可以使用在线工具<a href="https://www.matrixcalculus.org/">Matrix Calculus</a>求解。这里只展示出 损失 对 $W$ 的偏导数 来初步地理解梯度是如何计算的，因为损失对其他参数的偏导的计算方法都是一样的。</p>
<ol>
<li>
<p>首先求下式偏导
$$
\begin{equation}
\begin{aligned}
\frac{\partial {L}} {\partial{\hat{y}}} &amp;= 
\begin{bmatrix}
\frac{\partial {L}} {\partial{\hat{y}_1}} &amp; 
\frac{\partial {L}} {\partial{\hat{y}_2}} &amp;
\cdots &amp;
\frac{\partial {L}} {\partial{\hat{y}<em>i}} &amp;
\cdots &amp;
\frac{\partial {L}} {\partial{\hat{y}</em>{10}}}
\end{bmatrix} \</p>
<p>\end{aligned}
\end{equation}
$$
简单易得 
$$ \frac{\partial {L}} {\partial{\hat{y}_i}} = - \frac{y_i}{\hat{y}_i}  $$</p>
</li>
<li>
<p>然后我们在上一步中已经了解了 softmax 的 求导，并且 $ \hat{y} $ 就是 $s$， 因此我们直接有
$$
\frac{\partial{\hat{y}_i}}{\partial{z_j}}  = 
\begin{cases}
\hat{y}_i - {\hat{y}_i}{\hat{y}_j} &amp; \text{if } (i = j ) \
-{\hat{y}_i}{\hat{y}_j} &amp; \text{if } (i \neq j) \
\end{cases}
$$</p>
</li>
<li>
<h1>然后我们要求
$$
\begin{aligned}
\frac{\partial{z}}{\partial{W}} &amp;= 
\begin{bmatrix}
\frac{\partial{z_1}}{\partial{W}}  \[6pt]
\frac{\partial{z_2}}{\partial{W}} \[6pt]
\vdots \[6pt]
\frac{\partial{z_n}}{\partial{W}} \[6pt]
\end</h1>
<p>\begin{bmatrix}
\frac{\partial {z_1}} {\partial{W_{1,1}}} &amp; 
\frac{\partial {z_1}} {\partial{W_{1,2}}} &amp;
\cdots&amp;
\frac{\partial {z_1}} {\partial{W_{1,64}}}
\[6pt]</p>
<p>\frac{\partial {z_2}} {\partial{W_{2,1}}} &amp; 
\frac{\partial {z_2}} {\partial{W_{2,2}}} &amp;
\cdots&amp;
\frac{\partial {z_2}} {\partial{W_{2,64}}}
\[6pt]</p>
<h1>\vdots &amp;  \vdots &amp;   \frac{\partial {z_j}} {\partial{W_{j,k}}}   &amp;\vdots \[6pt]
\frac{\partial {z_{10}}} {\partial{W_{10,1}}} &amp; 
\frac{\partial {z_{10}}} {\partial{W_{10,2}}} &amp;
\cdots&amp;
\frac{\partial {z_{10}}} {\partial{W_{10,64}}}
\end{bmatrix}
\end{aligned}
$$
此时为了求出 $\frac{\partial {z_j}} {\partial{W_{j,k}}}$， 我们首先需要把 $z =  W h + b$ 展开来看，如下所示
$$
\begin{bmatrix}
z_1 \ z_2 \  \vdots\ z_j\ \vdots\ z_{10}\
\end</h1>
<p>\begin{bmatrix}
W_{1, 1} &amp; W_{1, 2} &amp; \cdots &amp;\cdots &amp;\cdots &amp; W_{1, 64} \
W_{2, 1} &amp; W_{2, 2} &amp; \cdots &amp;\cdots &amp;\cdots &amp; W_{2, 64} \</p>
<p>\vdots &amp;\vdots   &amp; &amp; &amp; &amp;\vdots \
W_{j, 1} &amp;W_{j, 2} &amp;\cdots &amp;W_{j, k} &amp;\cdots &amp; W_{j, 64}  \
\vdots &amp;\vdots   &amp; &amp; &amp; &amp; \vdots  \
W_{10, 1} &amp; W_{10, 2} &amp; \cdots &amp;\cdots  &amp;\cdots &amp; W_{10, 64}\
\end{bmatrix}</p>
<p>\begin{bmatrix}
h_1 \ h_2 \  \vdots\ h_k\ \vdots\ h_{64}\
\end{bmatrix}</p>
<ul>
<li></li>
</ul>
<p>\begin{bmatrix}
b_1 \ b_2 \  \vdots\ b_j\ \vdots\ b_{64}\
\end{bmatrix}
$$
因此我们有
$$z_j = \sum_{k=1}^{64} W_{j,k} h_k$$
故而可以求得
$$ \frac{\partial{z_j} }{ \partial{W_{j,k}}} = h_k $$</p>
</li>
</ol>
<p>$$
\begin{equation}
\frac{\partial {L}} {\partial{W}} = </p>
<p>\begin{bmatrix}
\frac{\partial {L}} {\partial{W_{1,1}}} &amp; 
\frac{\partial {L}} {\partial{W_{1,2}}} &amp;
\cdots&amp;
\frac{\partial {L}} {\partial{W_{1,64}}}
\[6pt]</p>
<p>\frac{\partial {L}} {\partial{W_{2,1}}} &amp; 
\frac{\partial {L}} {\partial{W_{2,2}}} &amp;
\cdots&amp;
\frac{\partial {L}} {\partial{W_{2,64}}}
\[6pt]</p>
<p>\vdots &amp;  \vdots &amp;   \frac{\partial {L}} {\partial{W_{j,k}}}   &amp;\vdots \[6pt]
\frac{\partial {L}} {\partial{W_{10,1}}} &amp; 
\frac{\partial {L}} {\partial{W_{10,2}}} &amp;
\cdots&amp;
\frac{\partial {L}} {\partial{W_{10,64}}}
\end{bmatrix} \in \mathbb{R}^{10 \times 64}
\end{equation}
$$</p>
<!-- 然后我们只需要求出  $\frac{\partial {L_i}} {\partial{W_{j,k}}}, (i,j \in [1, 10], k \in [1, 64]) $  就可以 -->
<p>最后我们就得到损失对W的偏导数 $\frac{\partial {L}} {\partial{W}} \in \mathbb{R}^{10 \times 64} $</p>
</li>
<li>
<p>最后通过获得的梯度来更新模型的参数</p>
<p>同样的，这里只介绍对参数 W 的过程，因为模型对其他的参数 W1, b的更新过程是类似的。</p>
<ol>
<li>在利用pytorch定义好自己的模型后，模型参数 一开始是都是随机生成的，其随机性由pytorch的随机种子来确定。一般来说，对于全连接层，pytorch官方默认利用 均匀分布 来生成权重 W 的参数。在生成好 W 的参数后，这时如果直接用刚初始化的模型去预测手写数字的话，准确率是很低的。</li>
<li>接着我们把一张张的图片送入到模型当中去，然后会得到损失，进而我们可以得到损失对 W 的梯度，从而去更新 W 的参数，如下式所示，其中 lr 表示学习率，这是一个需要调节的超参数。（注意，这只是一个最简单形式的梯度下降更新过程，实际应用中会使用到更为复杂的优化器，比如我们在 手写数字识别 那一篇文章当中用的是Adam优化器）。
$$
\begin{equation}
W = W -  lr * \frac{\partial{L}}{\partial{W}}
\end{equation}
$$</li>
<li>之后不断地迭代去更新模型的参数，模型就能能够越来越准确地识别出手写的数字。</li>
</ol>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../deeplearning/4.交叉熵损失函数.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../misc/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../deeplearning/4.交叉熵损失函数.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../misc/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
