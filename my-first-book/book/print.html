<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="deeplearning/1.深度学习入门介绍.html">深度学习简要介绍</a></li><li class="chapter-item expanded "><a href="deeplearning/2.手写数字识别.html"><strong aria-hidden="true">1.</strong> 从手写数字识别入门深度学习</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deeplearning/3.多层感知机(MLP)解析.html"><strong aria-hidden="true">1.1.</strong> 多层感知机(MLP)解析</a></li><li class="chapter-item expanded "><a href="deeplearning/4.交叉熵损失函数.html"><strong aria-hidden="true">1.2.</strong> 交叉熵损失函数介绍</a></li><li class="chapter-item expanded "><a href="deeplearning/5.反向传播算法.html"><strong aria-hidden="true">1.3.</strong> 反向传播算法介绍</a></li></ol></li><li class="chapter-item expanded "><a href="misc/index.html"><strong aria-hidden="true">2.</strong> 杂项记录</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="misc/1.vscode_ssh.html"><strong aria-hidden="true">2.1.</strong> VSCode远程连接实验室windows电脑</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h2 id="什么是机器学习与深度学习"><a class="header" href="#什么是机器学习与深度学习"><strong>什么是机器学习与深度学习</strong></a></h2>
<p>机器学习和深度学习本质上是对数据的分析和处理，发现其中的规律、特征或模式，并将其用于分类、识别、预测等目的的一种技术。其中机器学习是利用大量的先验性知识手工提取特征，然后送入到传统的算法进行检测，比如支持向量机（Support Vector Machine, SVM) 、逻辑回归等；深度学习则往往不需要人工加入大量的先验知识，而是让神经网络模型 ( 比如多层感知机（Multilayer Perceptron, MLP）, 卷积网络（Convolutional Neural Networks, CNN ）等）见到大量的样本，并从中自动地归纳并提取出该种类型的样本的通用特征。</p>
<h2 id="深度学习算法框架选择"><a class="header" href="#深度学习算法框架选择"><strong>深度学习算法框架选择</strong></a></h2>
<p>深度学习算法方向众多，包括很多方面，比如</p>
<ul>
<li>
<p>计算机视觉（Computer Vision, CV）：计算机视觉是通过算法模型使得计算机能够像人类一样去识别各种各样的物体，比如猫、狗、车等众多物体。该领域包图像分类、目标检测、语义分割、图像生成等众多子领域。</p>
</li>
<li>
<p>自然语言处理（Natural Language Processing, NLP）：NLP旨在让计算机能够理解人类的语言，包括不限于机器翻译、文本分类、文本生成等众多领域。其中最火热的OpenAI的ChatGPT模型就是一个非常大的语言处理模型，它能够在一定程度上理解人类的问题，并给出答案。此外，它还具备文本总结归纳、文本翻译、文本生成等能力。</p>
</li>
<li>
<p>信号处理（Signal Processing）：信号处理是一种通过对信号进行处理和分析来提取信号特征的算法。主要包括语音识别、脑电波信号检测、雷达信号检测、机器运转故障检测等。其中脑电波信号检测可能是未来发展的一个大方向，比如马斯克创办的Neuralink就是主打创造微型的侵入式设备来获取高信噪比的脑电信号，为算法精准地识别到各类脑电信号提供了可能性，从而实现各种各样的应用，比如利用意念精准快速地打字、交流，意念移动物体等超科幻任务。</p>
</li>
<li>
<p>强化学习（Reinforcement Learning, RL）：强化学习是一种通过与环境交互来学习如何做出决策的机器学习方法。它主要应用于游戏、机器人控制、自动驾驶等领域。</p>
</li>
</ul>
<h2 id="算法方向"><a class="header" href="#算法方向"><strong>算法方向</strong></a></h2>
<p>深度学习是一门涵盖广泛的领域，其中包含着许多不同的子领域和技术。因此，选择一个合适的子领域并投入深入学习是非常重要的。但是不同的子领域之间也存在一定的联系和迁移性。例如，计算机视觉中的卷积神经网络（CNN）可以应用于语音识别中的声学模型，以及检测脑电信号等应用；自然语言处理中的循环神经网络（RNN）可以应用于时间序列预测中的模型。因此，在学习一个子领域的同时，也可以关注其他领域的发展和应用，以便更好地掌握整个深度学习领域。</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="手写数字数据集mnist介绍"><a class="header" href="#手写数字数据集mnist介绍"><strong>手写数字数据集MNIST介绍</strong></a></h2>
<p>手写数字数据集MNIST是一个常见的数字图片数据集，其包含了大量手写数字的灰度图像，每张图像的大小都是28x28像素。MNIST数据集有60000张图像用于训练和10000张图像用于测试，其中每张图像都被标记了对应的数字（0-9）。这个数据集非常简单，常常作为初学者的第一个 “Hello World” 项目。
<img src="deeplearning/./images/1.png" alt="" /></p>
<!-- more -->
<h2 id="上手操作"><a class="header" href="#上手操作"><strong>上手操作</strong></a></h2>
<p>为了方便初学者更好的学习，本项目采用编辑器VSCode和Python语言编写。</p>
<p>导入基本库，并加载MNIST数据集</p>
<pre><code class="language-python">import torch
import time
import torchvision
from torch.utils import data
from torch.autograd import Variable
from torchvision.datasets import mnist
import matplotlib.pyplot as plt
from torch import nn
import numpy as np
import itertools


'''固定随机种子，使得程序结果尽可能保证复现效果'''
seed = 1
torch.manual_seed(seed)


'''MINIST数据集预处理'''
data_tf = torchvision.transforms.Compose(
    [   
        #将原始数据转为tensor
        torchvision.transforms.ToTensor(), 
        # z-score 归一化，把MNIST中图片的像素分布拉回到 标准正态分布 N ~ (0, 1)
        torchvision.transforms.Normalize([0.1307],[0.3081]), 
    ]                                               
)


'''下载数据集'''
data_path =  'F:\\zhendianyuanzi_linux\\dataset\\images' # 自定义图片下载存放路径
train_data = mnist.MNIST(data_path, train=True, transform=data_tf, download = True)
test_data  = mnist.MNIST(data_path, train=False,transform=data_tf, download = True)
</code></pre>
<p>然后我们查看一下train_loader中的第一个batch中存放了什么</p>
<pre><code class="language-python">'''创建dataloader'''
batch_size = 128  #这里我们定义一个batch中存放128张图片
train_loader = data.DataLoader(train_data,batch_size=batch_size,shuffle=True) 
test_loader  = data.DataLoader(test_data, batch_size=batch_size,shuffle=False)

'''查看train_loader'''
for batch, (X, y) in enumerate(train_loader):
    #打印第一个batch中的shape
    print('X.shape:{}, y.shape:{}'.format(X.shape, y.shape))
    
    #显示第一batch中的第一张图片，和打印其对应的标签
    plt.imshow(X[0, 0], cmap = 'gray')
    print(y[0])
    break
</code></pre>
<p>运行这一行程序后，结果如下图所示。X表示一个batch中的样本，其shape为（128, 1, 28, 28），表示一个batch中一共有128个张图片，每张图片的通道数为1，每张图片对应的像素点个数是28x28。因为图片是灰度图像，所以只有1个通道，如果图片是彩色的，那么图片就有3个通道（即RGB）。y表示一个batch中样本所对应的标签，其值范围为从0到9。并且我们还把该batch中的第一张图片打印了出来，该图片是数字3，其对应的标签也是3。在读者的电脑上运行的时候，所显示的图片可能不是数字3，而是其他的数字。这是因为在这一句话中train_loader = data.DataLoader(train_data,batch_size=batch_size,shuffle=True)，我们开启了shuffle，值为True，这会让batch中的样本随机打乱，不固定。对于训练集train_loader中的样本必须要随机打乱，这样才能让模型更好地学习，并收敛。
<img src="deeplearning/./images/2.png" alt="" /></p>
<p>接下来我们定义网络模型结构，网络模型初步选择多层感知机（MLP）。</p>
<pre><code class="language-python">'''定义网络结构'''
class MLP(nn.Module):
    def __init__(self, hidden_dim, class_num):
        super(MLP, self).__init__()
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, hidden_dim),
            nn.Sigmoid(),
            nn.Linear(hidden_dim, class_num),
        )

    def forward(self, x):
        out = self.fc(x)
        return out

hidden_dim = 64 #隐藏层神经元个数
class_num  = 10 #由数字0到9，所以一共是10个类别
device = 'cuda' #使用 GPU设备，如果要使用cpu，请更改为 'cpu'
model = MLP(hidden_dim, class_num).to(device) # 把模型从内存中搬运到GPU中运行
print(model)
</code></pre>
<p>该模型的结构如下图所示
<img src="deeplearning/./images/3.png" alt="" /></p>
<p>定义训练函数train，和验证函数valid</p>
<pre><code class="language-python">def train(model, dataloader, loss_fn, optimizer, device):
    '''模型训练'''
    model.train() #很重要，开启模型的训练模式，保证batchnorm和dropout的参数是进行训练计算的
    loss_sum = 0; num_batches = 0
    y_true = []; y_pred = [] # y_true 和  y_pred 分别是真实的标签和模型预测的标签
    
    for batch, (X, y) in enumerate(dataloader):
        '''把图片喂给模型'''
        X = X.to(device)
        y = y.to(device)
        pred = model(X)
        _loss = loss_fn(pred, y)

        '''反向传播，更新模型参数'''
        optimizer.zero_grad()
        _loss.backward()
        optimizer.step()

        '''自定义变量参数更新'''
        loss_sum += _loss.item()
        num_batches += 1
        y_true.append( y.cpu().numpy())
        y_pred.append( pred.argmax(1).cpu().numpy())

        '''每经过200个batch就打印一次损失 '''
        if batch % 200 == 0:
            loss, current = _loss.item(), batch * len(X)
            print(f&quot;loss: {loss:&gt;7f}  [{current:&gt;5d}/{len(dataloader.dataset):&gt;5d}]&quot;)

    '''展平, 并将List变为Numpy数组'''
    y_true = np.array(list(itertools.chain.from_iterable(y_true)))
    y_pred = np.array(list(itertools.chain.from_iterable(y_pred)))

    return loss_sum/num_batches, y_true, y_pred


def valid(model, dataloader, loss_fn, device):
    '''模型验证 '''
    model.eval() #很重要，固定模型中的batchnorm和dropout，不然在验证阶段模型的参数会变动。
    loss_sum = 0; num_batches = 0
    y_true = []; y_pred = []

    with torch.no_grad(): # 验证阶段要禁止梯度的传播
        for batch, (X, y) in enumerate(dataloader):
            '''把图片喂给模型'''
            X = X.to(device)
            y = y.to(device)
            pred = model(X)
            _loss = loss_fn(pred, y)

            '''自定义变量参数更新'''
            loss_sum += _loss.item()
            num_batches += 1
            y_true.append( y.cpu().numpy())
            y_pred.append( pred.argmax(1).cpu().numpy())

        '''展平, 并将List变为Numpy数组'''
        y_true = np.array(list(itertools.chain.from_iterable(y_true)))
        y_pred = np.array(list(itertools.chain.from_iterable(y_pred)))

    return loss_sum/num_batches, y_true, y_pred
</code></pre>
<p>定义超参数、损失函数和优化器。</p>
<pre><code class="language-python">lr = 1e-3    #初始学习率设置
max_epoch = 5   #训练迭代次数

loss_fn = nn.CrossEntropyLoss()  #使用交叉熵损失
optimizer = torch.optim.Adam(model.parameters(), lr=lr)  #使用Adam优化器优化模型参数
</code></pre>
<p>开始训练。为了更加直观的学习，我们把MNIST的测试集当做验证集来使用。注意在实际中，验证集和测试集是要分开使用的，在后面的章节中我会详细地介绍训练集、验证集、测试集三者之间的关系。</p>
<pre><code class="language-python">for t in range(max_epoch):
    print(f&quot;Epoch {t+1}\n-------------------------------&quot;)
    train_loss, train_y, train_pred_y = train(model, train_loader, loss_fn, optimizer, device)
    valid_loss, valid_y, valid_pred_y = valid(model, test_loader, loss_fn, device)
    
    train_acc = sum(train_y == train_pred_y)/len(train_y)*100
    valid_acc = sum(valid_y == valid_pred_y)/len(valid_y)*100
    print(&quot;train accuracy:{}%, valid accuracy:{}%&quot;.format(train_acc, valid_acc))
</code></pre>
<p>训练的结果如下图所示，训练到第5个epoch后，发现验证集的准确率达到了95.46%，效果看起来还可以接受。如果想要获得更好的准确率，可以尝试使用不同的超参数组合，例如MLP模型隐藏神经元中的个数，学习率、最大训练迭代次数等。
<img src="deeplearning/./images/4.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>在上一节的手写数字识别入门中我们使用了多层感知机来识别手写数字图片，实现了不错的分类效果，MLP在深度学习中经常被使用，在这一节中，笔者将介绍什么是MLP。
MLP是多层感知器（Multi-Layer Perceptron）的缩写，是一种基于前馈神经网络（Feedforward Neural Network）的机器学习模型。它通常由多个全连接层组成，每个全连接层都包含多个神经元，每个神经元都与前一层的所有神经元相连，并通过激活函数对输入进行非线性变换。</p>
<!-- more -->
<h2 id="神经元"><a class="header" href="#神经元"><strong>神经元</strong></a></h2>
<p>首先让我们观察一下一个单独的神经元，如下图所示</p>
<p><img src="deeplearning/./images/5.png" alt="" /></p>
<p>其中 $x_i$是 图片展平后的1维向量中的第 i 个点，我们假设这个1维的向量的长度为m。 $w_i$ 是神经元对感知到的第 i 个点的权重；b 表示该神经元的偏置。$w_i$ 和 b 表示该神经元的参数，这些参数的值的大小由反向传播更新获取。$\sigma$ 表示该神经元的激活函数。</p>
<p>$$\begin{equation}
y = \sigma( \sum_{i=1}^{m}  { w_i x_i} ) + b
\end{equation} $$</p>
<h2 id="激活函数"><a class="header" href="#激活函数"><strong>激活函数</strong></a></h2>
<p>激活函数有很多种，常见的有Sigmoid、ReLU、ELU、GELU等。这里我们介绍Sigmoid激活函数，这个激活函数在上一节 手写数字识别入门 中用到了。公式（2）是Sigmoid激活函数的数学表达式，用来激神经元，也就是把神经元的线性输出转变为非线性。
$$
\begin{equation}
\sigma(x)=\frac{1}{1+e^{-x}} , x \in (-\infty, \infty)
\end{equation}
$$
Sigmoid函数图像如下图</p>
<p><img src="deeplearning/./images/6.png" alt="" /></p>
<h2 id="mlp模型解析"><a class="header" href="#mlp模型解析"><strong>MLP模型解析</strong></a></h2>
<p>在手写数字识别入门中我们利用python程序成功地让MLP获得了识别手写数字的能力，并达到了95.46%的准确率。那时我们先把28x28的图片展平为一维的向量，即784个点，这784个点和隐藏层中的每一个神经元都连接上。与此同时，并利用Sigmoid函数激活隐藏中的每一个神经元。最后，因为我们是分类问题，一共有10种类别，所以此时输出层中一共有10个神经元，这10个神经元和隐藏层中的每个神经元都相连。这样当我们的MLP模型经过训练后，就能够实现手写数字的识别。<strong>非常重要的一点是</strong>，隐藏层中的神经元都使用了Sigmoid函数激活，而输出层中的神经元在我们自己定义的MLP模型中没有加入激活函数。其实这是我们使用了torch中的nn.CrossEntropyLoss()函数。torch中的nn.CrossEntropyLoss() 中不需要手动将模型输出进行 softmax 变换，因为这个函数会自动进行 softmax 变换。因此，只需要将模型的输出和真实标签作为输入传递给 nn.CrossEntropyLoss() 函数即可。
<img src="deeplearning/./images/3.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>在 多层感知机（MLP）解析 一文当中，我们详细地介绍了什么MLP模型的框架模型，此外我们还提到了一个重要的函数——torch中的nn.CrossEntropyLoss() 。这个就是在分类问题中经常用的损失函数——<strong>交叉熵损失</strong>。在介绍交叉熵损失之前，我们先补充一些基本知识。</p>
<!-- more -->
<h2 id="softmax函数"><a class="header" href="#softmax函数"><strong>Softmax函数</strong></a></h2>
<p>Softmax 函数是一种将向量映射为概率分布的函数，常用于分类任务中。它将一个 $k$维的向量 $o \in \mathbb{R}^k$ 作为输入。softmax 函数的计算方式如下：
$$
\begin{equation}
\hat{y}<em>i=\text{softmax}(o_i) = \frac{e^{o_i}}{\sum</em>{j=1}^{k} e^{o_j}}
\end{equation}
$$</p>
<p>其中，$o_i$ 表示向量 $o$ 的第$i$  个元素，$k$  表示向量 $o$ 的维度。softmax 函数将向量 $o$  中的每个元素 $o_i$ 进行指数运算，然后将所有指数的和作为分母，每个指数除以分母作为分子，即可得到每个元素的 softmax 值，即$\hat{y}_i$</p>
<p>$$
\begin{equation}
\sum_{i=1}^{k}{\hat{y}_i}=1
\end{equation}
$$</p>
<p>我们再重新看一下这个MLP模型。其输出层中的每一个输出神经元都有一个输出值$o_i$。softmax函数会把每一个输出值压缩到0到1的区间，并使得所有的被压缩后的的值之和等于 1，即式（2）所示。softmax 函数的输出可以被解释为一个概率分布，其中每个元素的值表示该元素对应类别的概率。在分类任务中，通常将 softmax 函数的输出作为模型的预测概率分布。
<img src="deeplearning/./images/3.png" alt="" /></p>
<h2 id="one-hot编码"><a class="header" href="#one-hot编码"><strong>one-hot编码</strong></a></h2>
<p>one-hot 编码是一种将离散变量表示为向量的方法，常用于机器学习和深度学习中。但是我们不能直接把这种离散的标签用于训练，而是首先需要这些离散的标签转为向量。怎么转换呢？我们知道手写数字一共有10个类别（标签），如下表中的第一行所示。在 one-hot 编码中，每个离散的标签需要被转换为一个向量，向量的长度为类别的个数，我们有10个类别，所以向量的长度为10。并且每个向量中中仅有一个元素为 1，其余元素为 0，这个元素为1的位置对应于该标签的序号；比如标签为2排在第3位，那么其对应的one-hot 向量 为 0010000000，依次类推就可以得到其他标签所对应的one-hot向量。
<img src="deeplearning/./images/7.png" alt="" /></p>
<h2 id="负对数似然损失negative-log-likelihood-loss-nll-loss"><a class="header" href="#负对数似然损失negative-log-likelihood-loss-nll-loss"><strong>负对数似然损失（negative log-likelihood loss, NLL loss）</strong></a></h2>
<p>NLL loss以用于衡量模型输出的概率分布与真实标签之间的差异。假设有 $k$ 个类别，模型输出的概率分布为 $\hat{y} = (\hat{y}_1, \hat{y}_2, ..., \hat{y}_k)$，其中 $\hat{y}<em>i$ 表示输入样本属于第$i$ 个类别的概率，真实标签所对应的one-hot向量 $y = (y_1, y_2, ..., y_k)$。则多分类问题中的NLL loss可以被定义为
$$
\begin{equation}
L(y, \hat{y}) = -\sum</em>{i=1}^k y_i \log \hat{y}_i
\end{equation}
$$
NLL loss的物理意义是，模型预测向量 $\hat{y}$ 和真实标签所对应的向量 $y$ 越相近，损失函数的值越小，表示模型的预测结果越接近真实标签。</p>
<p>现在让我们用一个具体的实例来看一下NLL loss是怎么计算的。比方说，当前我喂给MLP模型一个手写数字为3的样本，那么其对应的标签 $y$ 就是$(0,0,0,1,0,0,0,0,0,0)$，然后这个MLP模型的输出经过softmax变换后所得到的向量为 $\hat{y}$ ，假设此时 $\hat{y}$ 为$(0.0031, 0.0034, 0.1727, 0.6574, 0.0030, 0.0657, 0.0341, 0.00500, 0.0540, 0.0016)$，然后就可计算出 $\hat{y}$ 取了log后的值，从而就可以计算出MLP模型在这个样本上的NLL损失为0.4194。$\odot$ 表示矩阵的逐元素相乘(elemetwise multiplication)。这就是NLL loss 的计算，NLL loss的值越小，说明模型的预测结果越接近真实标签。</p>
<p>$$
\hat{y}=
\begin{bmatrix}
0.0031\0.0034\0.1727\0.6574\0.0030\0.0657\0.0341\0.0050\0.0540\0.0016\
\end{bmatrix},
log(\hat{y}) = 
\begin{bmatrix}
-5.7909\ -5.6830\ -1.7560\ -0.4194\ -5.8051\ -2.7225\ -3.3798\ -5.2994\ -2.9191\ -6.4277\
\end{bmatrix}
$$</p>
<p>$$
L(y, \hat{y})=
-1
\cdot<br />
\begin{bmatrix}
0\0\0\1\0\0\0\0\0\0
\end{bmatrix}<br />
\odot
\begin{bmatrix}
-5.7909\ -5.6830\ -1.7560\ -0.4194\ -5.8051\ -2.7225\ -3.3798\ -5.2994\ -2.9191\ -6.4277\
\end{bmatrix}
=0.4194
$$</p>
<h2 id="交叉熵损失"><a class="header" href="#交叉熵损失"><strong>交叉熵损失</strong></a></h2>
<p>交叉熵损失（Cross Entropy Loss）是一种广泛用于分类问题的损失函数，它的主要思想是通过比较模型的预测值与真实标签之间的差异，来度量模型的性能。在分类问题中，交叉熵损失通常用于衡量模型的输出与真实标签之间的差异。它的计算方式如下：</p>
<ul>
<li>
<p>首先，将模型的输出通过 softmax 函数转化为一个概率分布，即所有类别的概率之 和为 1。</p>
</li>
<li>
<p>然后，将真实标签转为one-hot编码。</p>
</li>
<li>
<p>最后，计算模型输出的概率分布与真实标签的 one-hot 编码的NLL loss。这个loss值可以用来度量模型输出与真实标签的差异，即模型预测值与真实标签之间的距离。</p>
</li>
</ul>
<p>这样我们就获得了交叉熵的loss，然后就可以根据这个loss用于模型参数的训练更新。</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="什么是反向传播"><a class="header" href="#什么是反向传播"><strong>什么是反向传播</strong></a></h2>
<p>反向传播（Backpropagation）是一种用于训练神经网络的优化算法，它通过计算损失函数对神经网络参数的梯度来更新参数，从而最小化损失函数。反向传播算法是一种基于梯度下降的优化算法，它可以有效地优化多层神经网络中的参数，让模型具备学习能力。</p>
<h2 id="sofmax求导"><a class="header" href="#sofmax求导"><strong>Sofmax求导</strong></a></h2>
<p>我们有 模型的逻辑输出 $ z = [z_1, z_2, ..., z_n]^T$, Softmax的输出为
$s = [s_1, s_2, ..., s_n]^T$</p>
<p>$$
\begin{equation}
s_i = \frac{e^{z_i}}{\sum_{j=1}^{n}{e^{z_j}}  }
\end{equation}
$$
然后我们要求解 $\frac{\partial{s}}{\partial{z}}$， 即sotmax的导数。Softmax的导数可以用Jacobian矩阵来表示，如下式所示。
$$
\begin{equation}
\begin{split}</p>
<p>\frac{\partial{s}}{\partial{z}} = 
\begin{bmatrix}
\frac{\partial{s_1}}{\partial{z}}  \[6pt]
\frac{\partial{s_2}}{\partial{z}} \[6pt]
\vdots \[6pt]
\frac{\partial{s_n}}{\partial{z}} \[6pt]
\end{bmatrix}</p>
<p>&amp;=
\begin{bmatrix}
\frac{\partial{s_1}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_1}}{\partial{z_n}}\[6pt]
\frac{\partial{s_2}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_2}}{\partial{z_n}}\[6pt]
\vdots &amp; \frac{\partial{s_i}}{\partial{z_j}}  &amp; \vdots \[6pt]
\frac{\partial{s_n}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_n}}{\partial{z_n}}\[6pt]
\end{bmatrix}, (i,j \in [1, n])
\end{split}
\end{equation}
$$</p>
<p>我们首先求解 $\frac{\partial{s_i}}{\partial{z_j}}$。
但是因为有指数e，不好求导，为了方便求导，可以先加入log (默认log的底数为e) 即
$$
\begin{equation}
\begin{split}
log(s_i) &amp;= log(\frac{e^{z_i}}{\sum_{j=1}^{n}{e^{z_j}}  }) \
&amp;= z_i - log(\sum_{j=1}^{n}{e^{z_j}}) \
\end{split}
\end{equation}
$$</p>
<p>然后我们求解  $\frac{\partial{log(s_i)}}{\partial{z_j}}$</p>
<p>$$
\begin{equation}
\begin{split}
\frac{\partial{log(s_i)}}{\partial{z_j}} = \frac{\partial{log(s_i)}}{\partial{s_i}} \cdot \frac{\partial{ s_i }}{\partial{z_j}}
= \frac{1}{ s_i } \cdot \frac{\partial{ s_i }}{\partial{z_j}}
\end{split}
\end{equation}
$$
因此有$ \frac{\partial{ s_i }}{\partial{z_j}} = \frac{\partial{log(s_i)}}{\partial{s_i}} \cdot  s_i $。 所以我们要求下面的式子
$$
\begin{equation}
\begin{split}
\frac{\partial{log(s_i)}}{\partial{z_j}} 
&amp;= \frac{\partial{ (z_i - log(\sum_{j=1}^{n}{e^{z_j}}) )  }}{\partial{z_j}}\
&amp;= \frac{\partial{ z_i}}{\partial{z_j}} - \frac{\partial{ log(\sum_{j=1}^{n}{e^{z_j}}) }}{\partial{z_j}} \
&amp;= \frac{\partial{ z_i}}{\partial{z_j}}  - \frac{\partial{ log(\sum_{j=1}^{n}{e^{z_j}}) }}{\partial{(\sum_{j=1}^{n}{e^{z_j}}) }} \cdot 
\frac{\partial{(\sum_{j=1}^{n}{e^{z_j}}) }}{\partial{z_j}} \
&amp;= \frac{\partial{ z_i}}{\partial{z_j}} - \frac{e^{z_j}}{\sum_{j=1}^{n}{e^{z_j}}}\
&amp;= \frac{\partial{ z_i}}{\partial{z_j}} - s_j
\end{split}
\end{equation}
$$</p>
<p>之后我们就可以得到
$$
\begin{equation}
\begin{split}
\frac{\partial{s_i}}{\partial{z_j}} &amp;=  s_i \cdot \frac{\partial{ z_i}}{\partial{z_j}} - s_i s_j
\end{split}
\end{equation}
$$</p>
<p>最后我们就可以得到, sotfma的导数。表达式如下：
$$
\begin{equation}
\begin{split}</p>
<p>\frac{\partial{s}}{\partial{z}} = 
\begin{bmatrix}
\frac{\partial{s_1}}{\partial{z}}  \[6pt]
\frac{\partial{s_2}}{\partial{z}} \[6pt]
\vdots \[6pt]
\frac{\partial{s_n}}{\partial{z}} \[6pt]
\end{bmatrix}</p>
<p>&amp;=
\begin{bmatrix}
\frac{\partial{s_1}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_1}}{\partial{z_n}}\[6pt]
\frac{\partial{s_2}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_2}}{\partial{z_n}}\[6pt]
\vdots &amp; \frac{\partial{s_i}}{\partial{z_j}}  &amp; \vdots \[6pt]
\frac{\partial{s_n}}{\partial{z_1}} &amp; \dots &amp; \frac{\partial{s_n}}{\partial{z_n}}\[6pt]
\end{bmatrix}, (i,j \in [1, n])
\end{split}
\end{equation}
$$</p>
<p>$$
\frac{\partial{s_i}}{\partial{z_j}}  = 
\begin{cases}
s_i - {s_i}{s_j} &amp; \text{if } (i = j ) \
-{s_i}{s_j} &amp; \text{if } (i \neq j) \
\end{cases}
$$</p>
<h2 id="反向传播的步骤"><a class="header" href="#反向传播的步骤"><strong>反向传播的步骤</strong></a></h2>
<p>让我们继续以这个MLP为例，来详细解析反向传播算法的具体计算过程。
<img src="deeplearning//images/3.png" alt="" /></p>
<ul>
<li>
<p>前向传播：将展平为1维的图片 $x \in \mathbb{R}^m$ 送入到MLP中去，计算出MLP的输出结果 $\hat{y} \in \mathbb{R}^{10}$。这个过程用公式来表示就是
$$
\begin{equation}
\begin{aligned}
h &amp;= \sigma(W_1 x) + b_1 \
z &amp;=  W h + b \
\hat{y} &amp;= softmax(z) = \frac{e^{z}}{\text{sum}({e^z}) }</p>
<p>\end{aligned}
\end{equation}
$$
其中 $W_1 \in \mathbb{R}^{64 \times m} $ 表示输入层和隐藏层之间的权重参数， $b_1 \in \mathbb{R}^{64} $ 表示隐藏层中的这64个神经元的偏置。$h \in \mathbb{R}^{64}$表示隐藏层所有神经元的输出，是一个64维度的向量。
$W \in \mathbb{R}^{10 \times 64} $ 表示输入层和隐藏层之间的权重参数， $b \in \mathbb{R}^{10} $ 表示隐藏层中的这10个神经元的偏置。$z \in \mathbb{R}^{10}$表示输出层所有神经元的逻辑输出，<strong>在Pytorch中使用nn.CrossEntropyLoss()时，一定要注意把逻辑输出 $z$送给交叉熵函数，因为该函数的内部已经调用了softmax函数</strong>。
$W_1$，$W$，$b_1$ 和 $b$ 都是通过训练更新得来的。$\sigma$ 是激活函数。</p>
</li>
<li>
<p>计算梯度</p>
<p>要想计算计算梯度，那么必须要获取到损失函数的损失值。我们在上一节中已经详细地解说了pytorch中的交叉熵函数包括3个部分，即 首先把MLP的逻辑输出通过sotfmax函数激活，得到概率预测值 $\hat{y}$，然后将真实标签转换为one-hot编码 $y$，最后计算 $\hat{y}$ 和 $y$ 之间的NLL损失，如下式所示。 
$$
\begin{equation}
L(y, \hat{y}) = -\sum_{i=1}^k y_i \log \hat{y}_i<br />
\end{equation}
$$</p>
<p>然后我们就可以通过NLL loss来获取到梯度值。
那么求谁的梯度呢？因为我们要优化的是$W_1, W, b_1, b$ 的参数值，所以当然试求 NLL Loss对 $W_1, W, b_1, b$ 的梯度。矩阵的偏导数求解可以使用在线工具<a href="https://www.matrixcalculus.org/">Matrix Calculus</a>求解。这里只展示出 损失 对 $W$ 的偏导数 来初步地理解梯度是如何计算的，因为损失对其他参数的偏导的计算方法都是一样的。</p>
<ol>
<li>
<p>首先求下式偏导
$$
\begin{equation}
\begin{aligned}
\frac{\partial {L}} {\partial{\hat{y}}} &amp;= 
\begin{bmatrix}
\frac{\partial {L}} {\partial{\hat{y}_1}} &amp; 
\frac{\partial {L}} {\partial{\hat{y}_2}} &amp;
\cdots &amp;
\frac{\partial {L}} {\partial{\hat{y}<em>i}} &amp;
\cdots &amp;
\frac{\partial {L}} {\partial{\hat{y}</em>{10}}}
\end{bmatrix} \</p>
<p>\end{aligned}
\end{equation}
$$
简单易得 
$$ \frac{\partial {L}} {\partial{\hat{y}_i}} = - \frac{y_i}{\hat{y}_i}  $$</p>
</li>
<li>
<p>然后我们在上一步中已经了解了 softmax 的 求导，并且 $ \hat{y} $ 就是 $s$， 因此我们直接有
$$
\frac{\partial{\hat{y}_i}}{\partial{z_j}}  = 
\begin{cases}
\hat{y}_i - {\hat{y}_i}{\hat{y}_j} &amp; \text{if } (i = j ) \
-{\hat{y}_i}{\hat{y}_j} &amp; \text{if } (i \neq j) \
\end{cases}
$$</p>
</li>
<li>
<h1>然后我们要求
$$
\begin{aligned}
\frac{\partial{z}}{\partial{W}} &amp;= 
\begin{bmatrix}
\frac{\partial{z_1}}{\partial{W}}  \[6pt]
\frac{\partial{z_2}}{\partial{W}} \[6pt]
\vdots \[6pt]
\frac{\partial{z_n}}{\partial{W}} \[6pt]
\end</h1>
<p>\begin{bmatrix}
\frac{\partial {z_1}} {\partial{W_{1,1}}} &amp; 
\frac{\partial {z_1}} {\partial{W_{1,2}}} &amp;
\cdots&amp;
\frac{\partial {z_1}} {\partial{W_{1,64}}}
\[6pt]</p>
<p>\frac{\partial {z_2}} {\partial{W_{2,1}}} &amp; 
\frac{\partial {z_2}} {\partial{W_{2,2}}} &amp;
\cdots&amp;
\frac{\partial {z_2}} {\partial{W_{2,64}}}
\[6pt]</p>
<h1>\vdots &amp;  \vdots &amp;   \frac{\partial {z_j}} {\partial{W_{j,k}}}   &amp;\vdots \[6pt]
\frac{\partial {z_{10}}} {\partial{W_{10,1}}} &amp; 
\frac{\partial {z_{10}}} {\partial{W_{10,2}}} &amp;
\cdots&amp;
\frac{\partial {z_{10}}} {\partial{W_{10,64}}}
\end{bmatrix}
\end{aligned}
$$
此时为了求出 $\frac{\partial {z_j}} {\partial{W_{j,k}}}$， 我们首先需要把 $z =  W h + b$ 展开来看，如下所示
$$
\begin{bmatrix}
z_1 \ z_2 \  \vdots\ z_j\ \vdots\ z_{10}\
\end</h1>
<p>\begin{bmatrix}
W_{1, 1} &amp; W_{1, 2} &amp; \cdots &amp;\cdots &amp;\cdots &amp; W_{1, 64} \
W_{2, 1} &amp; W_{2, 2} &amp; \cdots &amp;\cdots &amp;\cdots &amp; W_{2, 64} \</p>
<p>\vdots &amp;\vdots   &amp; &amp; &amp; &amp;\vdots \
W_{j, 1} &amp;W_{j, 2} &amp;\cdots &amp;W_{j, k} &amp;\cdots &amp; W_{j, 64}  \
\vdots &amp;\vdots   &amp; &amp; &amp; &amp; \vdots  \
W_{10, 1} &amp; W_{10, 2} &amp; \cdots &amp;\cdots  &amp;\cdots &amp; W_{10, 64}\
\end{bmatrix}</p>
<p>\begin{bmatrix}
h_1 \ h_2 \  \vdots\ h_k\ \vdots\ h_{64}\
\end{bmatrix}</p>
<ul>
<li></li>
</ul>
<p>\begin{bmatrix}
b_1 \ b_2 \  \vdots\ b_j\ \vdots\ b_{64}\
\end{bmatrix}
$$
因此我们有
$$z_j = \sum_{k=1}^{64} W_{j,k} h_k$$
故而可以求得
$$ \frac{\partial{z_j} }{ \partial{W_{j,k}}} = h_k $$</p>
</li>
</ol>
<p>$$
\begin{equation}
\frac{\partial {L}} {\partial{W}} = </p>
<p>\begin{bmatrix}
\frac{\partial {L}} {\partial{W_{1,1}}} &amp; 
\frac{\partial {L}} {\partial{W_{1,2}}} &amp;
\cdots&amp;
\frac{\partial {L}} {\partial{W_{1,64}}}
\[6pt]</p>
<p>\frac{\partial {L}} {\partial{W_{2,1}}} &amp; 
\frac{\partial {L}} {\partial{W_{2,2}}} &amp;
\cdots&amp;
\frac{\partial {L}} {\partial{W_{2,64}}}
\[6pt]</p>
<p>\vdots &amp;  \vdots &amp;   \frac{\partial {L}} {\partial{W_{j,k}}}   &amp;\vdots \[6pt]
\frac{\partial {L}} {\partial{W_{10,1}}} &amp; 
\frac{\partial {L}} {\partial{W_{10,2}}} &amp;
\cdots&amp;
\frac{\partial {L}} {\partial{W_{10,64}}}
\end{bmatrix} \in \mathbb{R}^{10 \times 64}
\end{equation}
$$</p>
<!-- 然后我们只需要求出  $\frac{\partial {L_i}} {\partial{W_{j,k}}}, (i,j \in [1, 10], k \in [1, 64]) $  就可以 -->
<p>最后我们就得到损失对W的偏导数 $\frac{\partial {L}} {\partial{W}} \in \mathbb{R}^{10 \times 64} $</p>
</li>
<li>
<p>最后通过获得的梯度来更新模型的参数</p>
<p>同样的，这里只介绍对参数 W 的过程，因为模型对其他的参数 W1, b的更新过程是类似的。</p>
<ol>
<li>在利用pytorch定义好自己的模型后，模型参数 一开始是都是随机生成的，其随机性由pytorch的随机种子来确定。一般来说，对于全连接层，pytorch官方默认利用 均匀分布 来生成权重 W 的参数。在生成好 W 的参数后，这时如果直接用刚初始化的模型去预测手写数字的话，准确率是很低的。</li>
<li>接着我们把一张张的图片送入到模型当中去，然后会得到损失，进而我们可以得到损失对 W 的梯度，从而去更新 W 的参数，如下式所示，其中 lr 表示学习率，这是一个需要调节的超参数。（注意，这只是一个最简单形式的梯度下降更新过程，实际应用中会使用到更为复杂的优化器，比如我们在 手写数字识别 那一篇文章当中用的是Adam优化器）。
$$
\begin{equation}
W = W -  lr * \frac{\partial{L}}{\partial{W}}
\end{equation}
$$</li>
<li>之后不断地迭代去更新模型的参数，模型就能能够越来越准确地识别出手写的数字。</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>记录一些杂事</p>
<div style="break-before: page; page-break-before: always;"></div><p>今天在用 NATAPP 远程连接实验室那台 windows电脑出了个小毛病，一直报这个错误 <strong>kex_exchange_identification: Connection closed by remote host</strong>。之前在linux系统上好好的，后来发现原来是在windows上 ssh服务没有开启。
为了能够成功在windows连接上 ssh，需要进行以下步骤。</p>
<h1 id="ssh服务在自己本地的电脑和远程的windows电脑都要安装并开启"><a class="header" href="#ssh服务在自己本地的电脑和远程的windows电脑都要安装并开启"><strong>SSH服务在自己本地的电脑和远程的windows电脑都要安装并开启</strong></a></h1>
<h2 id="windows检查ssh-服务是否安装"><a class="header" href="#windows检查ssh-服务是否安装">Windows检查SSH 服务是否安装</a></h2>
<p>用管理员的方式 打开 windows PowerShell，然后运行如下指令</p>
<pre><code class="language-bash">Get-WindowsCapability -Online | Where-Object Name -like 'OpenSSH*'
</code></pre>
<p>发现我的 openssh 之前安装过，如果windows之前没有安装openssh的话， state应该是 NotPresent 的。
<img src="misc/./images/8.PNG" alt="" /></p>
<h2 id="安装-ssh-client-和-ssh-service-服务"><a class="header" href="#安装-ssh-client-和-ssh-service-服务">安装 SSH client 和 SSH service 服务</a></h2>
<p>运行如下指令安装ssh</p>
<pre><code class="language-bash"># Install the OpenSSH Client
Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0

# Install the OpenSSH Server
Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0
</code></pre>
<h2 id="开启-ssh-服务"><a class="header" href="#开启-ssh-服务">开启 SSH 服务</a></h2>
<p>这一步很重要，之前我就遗漏了这一步，从而导致我在连接远程实验室电脑的时候出现了 <strong>kex_exchange_identification: Connection closed by remote host</strong> 这个错误。</p>
<pre><code class="language-bash"># Start the sshd service
Start-Service sshd

# OPTIONAL but recommended:
Set-Service -Name sshd -StartupType 'Automatic'

# Confirm the Firewall rule is configured. It should be created automatically by setup. Run the following to verify
if (!(Get-NetFirewallRule -Name &quot;OpenSSH-Server-In-TCP&quot; -ErrorAction SilentlyContinue | Select-Object Name, Enabled)) {
    Write-Output &quot;Firewall Rule 'OpenSSH-Server-In-TCP' does not exist, creating it...&quot;
    New-NetFirewallRule -Name 'OpenSSH-Server-In-TCP' -DisplayName 'OpenSSH Server (sshd)' -Enabled True -Direction Inbound -Protocol TCP -Action Allow -LocalPort 22
} else {
    Write-Output &quot;Firewall rule 'OpenSSH-Server-In-TCP' has been created and exists.&quot;
}
</code></pre>
<p>最后附上ssh安装成功效果图
<img src="misc/./images/9.PNG" alt="" /></p>
<h2 id="测试远程ssh连接"><a class="header" href="#测试远程ssh连接">测试远程ssh连接</a></h2>
<p>我们首先在远程的实验室的那台windows电脑上下载 <a href="https://natapp.cn/">NATAPP</a> 这个软件，然后按照官网的教程配置并运行，会出现如下的命令框。
<img src="misc/./images/10.PNG" alt="" />
我们只需要关注这一段内容即可</p>
<pre><code class="language-bash">tcp://server.natappfree.cc:44809 -&gt; 127.0.0.1:22
</code></pre>
<p>server.natappfree.cc 表示远程服务器的地址，44809 表示 开启的端口号。
127.0.0.1:22 不用管。</p>
<p>之后我们在本地命令行运行</p>
<pre><code class="language-bash">ssh admin@server.natappfree.cc -p 44809
</code></pre>
<p>其中admin 是我的实验室windows电脑的用户名。
之后会提示我们需要输入远程实验室windows电脑的密码
<img src="misc/./images/11.PNG" alt="" />
在输入完密码之后就完成ssh远程连接
<img src="misc/./images/12.PNG" alt="" /></p>
<h2 id="利用vscode进行ssh连接"><a class="header" href="#利用vscode进行ssh连接">利用Vscode进行ssh连接</a></h2>
<p>打开VScode，安装插件 Remote - SSH</p>
<p><img src="misc/./images/13.PNG" alt="" /></p>
<p>安装好后会在 vscode的最左下角，出现一个连接标志,然后我们点击这个标志</p>
<p><img src="misc/./images/14.PNG" alt="" /></p>
<p>出现
<img src="misc/./images/Picture1.png" alt="" /></p>
<p>点击 Connect to Host， 之后点击
Configure SSH Hosts，会出现
<img src="misc/./images/a_1.png" alt="" /></p>
<p>之后我们点击第一个，出现如下配置，这是我配置好的ssh文件
<img src="misc/./images/a_2.png" alt="" /></p>
<p>然后 把HostName， User， Port 根据需要修改成自己的配置，并保存</p>
<p>继续点击vscode最左下角的那个ssh连接图标，出现
<img src="misc/./images/a_3.png" alt="" /></p>
<p>点击 labcompupter后，vscode会提示我们输入密码，输入远程电脑的登录密码即可。最后附上连接成功的效果图
<img src="misc/./images/a_4.png" alt="" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
